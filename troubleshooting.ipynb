{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa2316d-5ad9-48ce-bfad-f2bd65377fbd",
   "metadata": {},
   "source": [
    "ENABLE SPARK LOGGING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058feb97-509d-44e9-8aef-0c1325ede741",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\") #all,debug,info,warn,error helps reduce or increase log verbosity for better debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e463f6ff-cc54-4b6c-9860-d65684e16dd5",
   "metadata": {},
   "source": [
    "2.CHECK SPARK CONFIGURATION AT RUNTIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800d95d-fbdf-49b3-b285-7ab4ee0d0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()#useful to verify memory ,cores,shuffle config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05a9d3-ed5a-490c-bf56-db5f88f6e670",
   "metadata": {},
   "source": [
    "3.EXECUTOR MEMORY DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7c41b9-b1d9-4eba-b2e8-8f1e5a9325c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "--conf spark.executor.memory=2g\n",
    "--conf spark.driver.memory=1g\n",
    "#used when facing out of memory errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae8de3-ae1d-4f22-a396-cf1f76e0e957",
   "metadata": {},
   "source": [
    "4.SHUFFLE DEBUGING--TOO MUCH SHUFFLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce1fd1-38d9-455a-aa24-30e2c7cc0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.repartition(100,\"key\")#before groupby or join avoids default small partitions that cause skew."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad5c643-1e5f-4c2b-bdb8-57d0157a989d",
   "metadata": {},
   "source": [
    "5.HANDLING SKEWED JOIN KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d20d6c-246c-4757-8556-473af4ccbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "result=big_df.join(broadcast(small_df),\"key\") #fixes long tasks due to skew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f30a6-b70e-4c03-b114-014d78bdef09",
   "metadata": {},
   "source": [
    "6.CATCH READ FAILURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c048a33-0a7b-489c-94c0-482ac2e0a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df=spark.read_csv(\"path/does/not/exist\")\n",
    "except Exceptions as e:\n",
    "    print(\"Failed to read:\",e) #good for detecting bad paths or file formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b835f16-3bd3-4158-84ca-33120821cc99",
   "metadata": {},
   "source": [
    "7.MEMORY USAGE VIA EXECUTORS TAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019f08c2-507c-4247-b7ab-bc9260a1a981",
   "metadata": {},
   "outputs": [],
   "source": [
    "Use Spark UI at localhost:4040 > Executors tab #diagnosis executor memory issues visibly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08369fa-cbd5-4a08-9d1b-6ad5b2c96592",
   "metadata": {},
   "source": [
    "8.MONITOR JOB EXECUTION TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b60344-22ce-440a-98e2-87ac6d32746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start=time()\n",
    "df.count()\n",
    "print(\"Execution Time:\",time()-start()# useful to measure slow operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca89249a-8917-4ec5-ba8e-fdca75138d8c",
   "metadata": {},
   "source": [
    "9.Use.persist() wisely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba924e-aa63-4eb8-827d-2f8acdbb83bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.filter(\"status='active'\").persist()#avoid caching row or large unfiltered data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d545eac0-8793-408b-a52d-63c5d9352065",
   "metadata": {},
   "source": [
    "10.analyse task duration\n",
    "view task skew in Spark UI> Stages>Tasks #useful to detect long running tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9059e97a-a3a0-425d-ae84-87cb3ceba38e",
   "metadata": {},
   "source": [
    "11.AVOID EXPLODING MEMORY IN COLLECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006da20e-5a40-4728-8554-a6af0fee432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bad\n",
    "all_rows=df.collect()\n",
    "#better\n",
    "df.show(10)\n",
    "#prevents memory issues on the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3e529-93f9-4eec-be29-42422efeb41f",
   "metadata": {},
   "source": [
    "12.CHECK NUMBER OF PARTITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da981d7-a37f-47f1-87a8-0ba145b8960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.rdd.getNumPartitions())\n",
    "#too few partitions cause bottlenecks.too many=overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7f3dee-47fc-4ded-867e-e9e691cefa11",
   "metadata": {},
   "source": [
    "13.DEBUG JOINS TYPS ISSUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afff4fe-a80c-488c-9de1-77cc9c9e27d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2,\"id\",\"inner\").explain(True)\n",
    "#check for broadcast hint or sort merge join issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583393e0-2f08-4d10-9d01-eb81210ad5a3",
   "metadata": {},
   "source": [
    "14.INVESTIGATE LAZY EVALUATION PROBLEMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c58c5-4c7c-4215-8d2c-396aac455cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()#doesnot trigger jobs\n",
    "df.count() #triggers full execution\n",
    "#use actions to debug transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07105d1b-0939-4ee2-a718-e8a5e2d9bec0",
   "metadata": {},
   "source": [
    "15.track failed jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38157d74-e6db-4cd9-b63c-7beb3a60cafc",
   "metadata": {},
   "source": [
    "Use Spark UI>Jobs tab>click failed jobs>review stderr logs\n",
    "best for tracking oom file read,schema,mismatch erros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
